{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "20e4c5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/sidsr/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/sidsr/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Number of sentences:  57340\n",
      "Example random sentence:  [('Then', 'ADV'), ('Mel', 'NOUN'), ('Chandler', 'NOUN'), ('started', 'VERB'), ('up', 'ADP'), ('the', 'DET'), ('hill', 'NOUN'), ('.', '.')]\n",
      "Train size: 48739, Val size: 5734, Test size: 2867\n",
      "Saved train-data.json\n",
      "Saved val-data.json\n",
      "Saved test-data.json\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import json, os\n",
    "import random\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# Download if not already\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "# Get POS-tagged sentences with the 'universal' tagset\n",
    "tagged_sentences = brown.tagged_sents(tagset='universal')\n",
    "\n",
    "print(f\"{'Number of sentences: ':>25}\", len(tagged_sentences))\n",
    "print(f\"{'Example random sentence: ':>25}\", tagged_sentences[random.randint(0, len(tagged_sentences)-1)])\n",
    "\n",
    "# Shuffle data for randomness\n",
    "random.shuffle(list(tagged_sentences))\n",
    "\n",
    "# Split sizes\n",
    "n_total = len(tagged_sentences)\n",
    "n_train = int(0.85 * n_total)\n",
    "n_val = int(0.10 * n_total)\n",
    "n_test = n_total - n_train - n_val  # to cover rounding\n",
    "\n",
    "train_set = tagged_sentences[:n_train]\n",
    "val_set = tagged_sentences[n_train:n_train+n_val]\n",
    "test_set = tagged_sentences[n_train+n_val:]\n",
    "\n",
    "print(f\"Train size: {len(train_set)}, Val size: {len(val_set)}, Test size: {len(test_set)}\")\n",
    "\n",
    "# Convert to serializable format\n",
    "def convert(data):\n",
    "    return [[{word: tag} for word, tag in sentence] for sentence in data]\n",
    "\n",
    "# Save to JSON files\n",
    "splits = {\n",
    "    \"train-data.json\": convert(train_set),\n",
    "    \"val-data.json\": convert(val_set),\n",
    "    \"test-data.json\": convert(test_set),\n",
    "}\n",
    "\n",
    "for path, data in splits.items():\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(data, f, indent=2)   # indent=2 for readability\n",
    "    print(f\"Saved {path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "this_env (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
